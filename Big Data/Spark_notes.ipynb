{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Notes for Beginners\n",
    "usefull links:\n",
    "* https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html\n",
    "* https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html\n",
    "* https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_ps.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Start-up and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Spark Start-up\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession().builder.appName(\"Spark_notes\").getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\"/home/jovyan/work/data/iris.csv\", inferSchema=True, header=True) #read classes: csv,json, table, parquet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Basic Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Spark Apis\n",
    "\n",
    "df.show() #or z.show(df)\n",
    "\n",
    "## to see the column infos\n",
    "df.printSchema()\n",
    "\n",
    "## to see just column names\n",
    "df.columns\n",
    "\n",
    "##describe the dataframe like pandas\n",
    "df.describe().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Spark Datatypes\n",
    "\n",
    "from pyspark.sql.types import StringType,StructField,IntegerType,StructType, FloatType, DoubleType, NullType, BooleanType, DateType, TimestampType, BinaryType, ArrayType\n",
    "\n",
    "## change the data type of a column in a dataframe [age, name]\n",
    "data_schema = StructType(fields=[StructField(\"age\", IntegerType(), True),StructField(\"name\", StringType(), True)])\n",
    "\n",
    "new_df = spark.createDataFrame([(1, \"John\"), (2, \"Smith\")], data_schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Spark dataframe cell reference\n",
    "\n",
    "\n",
    "df['column_name'] #it just returns a column type not values inside a column\n",
    "\n",
    "## to see values inside a column (somethink like pandas.series)\n",
    "df.select(df.age).show() #or df.select('age').show()\n",
    "\n",
    "## to see values inside multiple columns(like df['column1','column2'])\n",
    "df.select(df.age,df.name).show()\n",
    "\n",
    "## rename a column\n",
    "df.withColumnRenamed('age','age_new').show()\n",
    "\n",
    "########################################################################################################################\n",
    "## to see top values inside a row (somethink like df.head())\n",
    "df.head(1:x)\n",
    "#this object can be indexed like a list\n",
    "df.head(5)[4]  # returns df.iloc[4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Something like apply function in pandas\n",
    "\n",
    "new_df = df.withColumn('new_column', df.age + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Spark Filter\n",
    "\n",
    "df.filter(\"age > 20\").show() # spark.filter is able to work  like sql where\n",
    "df.filter(df.age > 20).show() # also it is possible to call it like pandas.filter\n",
    "# PERSONAL NOTE: instead of using where in sql query use sql filter as above. VERYYYY FASTERRRR\n",
    "\n",
    "## apis can be used to gether like:\n",
    "df.filter(df.age > 20).filter(df.name == 'John').show()\n",
    "df.filter((df.age > 20) & ~(df.name == 'John')).show() #and: &, or: |, not: ~\n",
    "df.filter(df.age>20).select(df.name).show()\n",
    "\n",
    "########################################################################################################################\n",
    "## result of a filter can be saved and used like a list\n",
    "filter_result = df.filter(df.age > 20).collect() #collect() returns a list of rows\n",
    "\n",
    "#there are a lot we can do with this list for example:\n",
    "filter_result[0][0] #returns the first element of the first row\n",
    "#functions can be used like:  as_dict, count, index\n",
    "filter_result[0].asDict() #returns the first row as a dictionary \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Groupby, Agg and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Spark GroupBy \n",
    "df.groupBy('name').count().show() #count, max, mean, min, sum, avg, collect_list, collect_set, collect_as_dict\n",
    "\n",
    "df.agg({\"age\": \"max\"}).show() #agg functions are same as groupby functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instead of using agg functions we can use:\n",
    "from pyspark.sql.functions import countDistinct, stddev, avg, format_number\n",
    "\n",
    "df.select(avg('age').alias(\"age_average\")).show() #alias is something like 'as' in sql\n",
    "\n",
    "df.select(format_number( avg('age'),2 )).show() #format_number is used to show float numbers with n decimal places , here n is 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Spark DateTime and Timestamp\n",
    "from pyspark.sql.functions import dayofmonth,dayofweek,dayofyear,month,year,hour,minute,second,weekofyear,date_format\n",
    "#example:\n",
    "df.select(dayofmonth(df.date)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Spark OrderBy\n",
    "df.orderBy(df['age'].desc()).show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Null Value Handlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.na.drop(thresh=2).show() #drop rows with more than 2 null values\n",
    "df.na.drop(how='any').show() #any: drop rows with any null values,all: drop rows with all null values\n",
    "df.na.drop(subset=['name']).show() #drop rows with null values in a specific column\n",
    "\n",
    "########################################################################################################################\n",
    "df.na.fill(valeu=0,subset=['numerical_columns']).show() #fill null values in nu,eric cols with 0\n",
    "#example:\n",
    "mean_age = df.select(avg(df['age'])).collect()[0][0]\n",
    "df.na.fill(value=mean_age,subset=['age']).show() \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save a dataframe in hdfs as a view\n",
    "df.createOrReplaceTempView(\"my_table\")\n",
    "\n",
    "view_df = spark.sql(\"select * from my_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "04ffa89eceaafa2a0436d0fe8747e77d65eb76fa0b2821a79eee42bf759607c7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tests')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
