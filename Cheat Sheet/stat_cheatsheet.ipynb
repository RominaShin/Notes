{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concepts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* PMF:\n",
    "<br>\n",
    "Another way to represent a distribution is a probability mass function (PMF), which maps from each value to its probability.  \n",
    "useful for discrete features.\n",
    "\n",
    "\n",
    "* CDF:  \n",
    "maps from each value to its percentile rank  \n",
    "The CDF gives the probability that a random variable is less than or equal to a certain value. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* PDF:  \n",
    "It is a function that describes the probability distribution of a continuous random variable.  \n",
    "The PDF is used to calculate the probability of a random variable taking on a specific value or falling within a certain range of values.  \n",
    "The PDF is defined as the derivative of the cumulative distribution function (CDF) of a random variable.  The PDF gives the rate at which the CDF changes with respect to the random variable.  \n",
    "The PDF has several important properties, including:\n",
    "\n",
    "\n",
    "    - The area under the PDF curve is equal to 1, since the total probability of all possible outcomes is 1.\n",
    "    - The PDF is non-negative, since the probability of a random variable taking on a negative value is zero.\n",
    "    - The PDF can be used to calculate the expected value and variance of a random variable.\n",
    "    - The PDF can be used to calculate the probability of a random variable falling within a certain range of values, by integrating the PDF over that range.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Kernel density estimation (KDE) is an algorithm that takes a sample and finds an appropriately smooth PDF that fits the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* PDFS:\n",
    "    - gaussian dist: \n",
    "        + z-score: standardize data\n",
    "        + QQ-plot: A plot to visualize how close a sample distribution is to a specified distribution\n",
    "        + Unimodal -one mode\n",
    "        + Symmetrical -left and right halves are mirror images\n",
    "        + Bell-shaped -maximum height (mode) at the mean\n",
    "        + Mean, Mode, and Median are all located in the center\n",
    "        + Asymptotic(no bias to left or right)\n",
    "        + 68% of the data falls within one standard deviation of the mean or median. \n",
    "        if the median of a dataset is 10 and the standard deviation is 2, \"1 standard deviation from the median\" would refer to the range of values from 8 to 12, since these values are one standard deviation away from the median of 10.\n",
    "    - uniform dist\n",
    "    - exponential dist\n",
    "    - lognormal dist"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Centeral Limit Theorem:  \n",
    "<br>\n",
    "this rule says mean of each dataset comes from a normal dist.  \n",
    "if we take many samples from our data and calculate mean of samples. means will make a normal dist.\n",
    "<br>\n",
    "now if we take interval which includes 95% of the means, we have 95% confidence interval.  \n",
    "it means anything out of this interval is significantly different from data.  \n",
    "or if confidence intervals of two samples of data don't overlap, they are significantly different.    \n",
    "useful when we don't know what dist our data is coming from.   \n",
    "so we calc mean, find confidence interval, do t-test or ANOva to find out if ther's difference between two or more samples."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* P-value:\n",
    "<br>\n",
    "the purpose is minimizing effects of random things on an event.\n",
    "<br>\n",
    "the more closer the p-value is to 0, the more we are confident that two samples are different.\n",
    "p-value of 0.05 means we may get 5 false positive in every 100 cases.\n",
    "alpha = significance level = 0.05\n",
    "<br>\n",
    "p-value helps to reject null hypothesis.\n",
    "<br>\n",
    "the problem with p-value is it doesn't include the size of samples. so we need effect size\n",
    "\n",
    "* Calculating p-value:\n",
    "prob of an event + prob of event equally rare(area under dist) + prob of an event rarer (area under dist)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Power:  \n",
    "the probability of a hypothesis test of finding an effect, if there is an effect to be found?  \n",
    "or the probability of getting correctly small p-value, or correctly reject null hypothesis \n",
    "\n",
    "\n",
    "* power analysis:  \n",
    "tells how much sample size we "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Compared to the variance of the Maximum Likelihood Estimate (MLE), the variance of the Maximum A Posteriori (MAP) estimate is Lower.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Bayes’ Theorem gives you the posterior probability of an event given what is known as prior knowledge.  \n",
    "Mathematically, it’s expressed as the true positive rate of a condition sample divided by the sum of the false positive rate of the population and the true positive rate of a condition. Say you had a 60% chance of actually having the flu after a flu test, but out of people who had the flu, the test will be false 50% of the time, and the overall population only has a 5% chance of having the flu. Would you actually have a 60% chance of having the flu after having a positive test?  \n",
    "Bayes’ Theorem says no. It says that you have a (.6 * 0.05) (True Positive Rate of a Condition Sample) / (.60.05)(True Positive Rate of a Condition Sample) + (.50.95) (False Positive Rate of a Population) = 0.0594 or 5.94% chance of getting a flu.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Tests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __T-test (Independent and Paired)__:  \n",
    " Used to compare the means of two groups or samples. Independent t-test is for comparing two independent   groups, while the paired t-test is for comparing two related groups (e.g., before and after measurements).\n",
    "    + Assumes normality of the data.\n",
    "    + Assumes homogeneity of variances (equal variances) for the independent t-test.\n",
    "    + Sensitive to outliers.\n",
    "    + Not suitable for categorical data.\n",
    "- __ANOVA (Analysis of Variance)__:  \n",
    " Used to compare the means of three or more groups or samples. One-way ANOVA is for comparing groups with a   single independent variable, while two-way ANOVA is for comparing groups with two independent variables.\n",
    "    + Assumes normality of the data.\n",
    "    + Assumes homogeneity of variances (equal variances) among groups.\n",
    "    + Sensitive to outliers.\n",
    "    + Not suitable for categorical data.\n",
    "- __Chi-square test__:  \n",
    " Used to test the association between two categorical variables in a contingency table.  \n",
    "\n",
    "    + Requires categorical data.\n",
    "    + Assumes that observations are independent.\n",
    "    + May not be accurate for small sample sizes or when expected frequencies are very low.\n",
    "- __Fisher's exact test__:  \n",
    " Similar to the Chi-square test, but used when the sample size is small, and the Chi-square test assumptions may not hold. \n",
    "\n",
    "    + Requires categorical data.\n",
    "    + Assumes that observations are independent.\n",
    "    + Computationally intensive for large sample sizes or large contingency tables.\n",
    "- __Mann-Whitney U test__:  \n",
    " A non-parametric test used to compare the distributions of two independent groups when the data is not normally   distributed.\n",
    "\n",
    "    + Assumes that the distributions of both groups are similar in shape.\n",
    "    + Not suitable for paired or related samples.\n",
    "    + Less powerful than parametric tests when data is normally distributed.\n",
    "- __Wilcoxon signed-rank test__:  \n",
    " A non-parametric test used to compare the distributions of two related groups when the data is not normally   distributed.\n",
    "\n",
    "    + Assumes that the differences between paired observations are symmetric.\n",
    "    + Less powerful than parametric tests when data is normally distributed.\n",
    "- __Kruskal-Wallis test__:  \n",
    " A non-parametric test used to compare the distributions of three or more independent groups when the data is not normally   distributed.\n",
    "\n",
    "    + Assumes that the distributions of all groups are similar in shape.\n",
    "    + Less powerful than parametric tests when data is normally distributed.\n",
    "- __Spearman's rank correlation__:  \n",
    " A non-parametric measure of the strength and direction of the association between two ranked variables.  \n",
    "\n",
    "    + Assumes a monotonic relationship between variables.\n",
    "    + Less powerful than Pearson's correlation when the relationship is linear.\n",
    "- __Pearson's correlation__:  \n",
    " A measure of the strength and direction of the linear relationship between two continuous variables.  \n",
    "\n",
    "    + Assumes a linear relationship between variables.\n",
    "    + Sensitive to outliers.\n",
    "    + Not suitable for non-linear relationships or non-continuous data.\n",
    "- __Linear regression__:  \n",
    " A method for modeling the relationship between a dependent variable and one or more independent variables.  \n",
    "\n",
    "    + Assumes a linear relationship between dependent and independent variables.\n",
    "    + Assumes normality of residuals.\n",
    "    + Assumes homoscedasticity (constant variance of residuals).\n",
    "    + Sensitive to outliers and multicollinearity.\n",
    "- __Logistic regression__:  \n",
    " A method for modeling the probability of a binary outcome based on one or more independent variables.  \n",
    "\n",
    "    + Assumes a linear relationship between the logit of the outcome and the independent variables.\n",
    "    + Requires a large sample size for accurate estimates.\n",
    "    + Not suitable for continuous outcomes or non-binary categorical outcomes.\n",
    "- __Time series analysis__:\n",
    "    + Assumes that the time series is stationary or can be made stationary through transformations.\n",
    "    + Requires a sufficiently long time series for accurate modeling.\n",
    "    + May not be suitable for time series with complex or non-linear patterns."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions\n",
    "\n",
    "1. __In an A/B test, how can you check if assignment to the various buckets was truly random?__  \n",
    "Plot the distributions of multiple features for both A and B and make sure that they have the same shape.  \n",
    "More rigorously, we can conduct a permutation test to see if the distributions are the same.  \n",
    "MANOVA to compare different means\n",
    "\n",
    "2. __How would you run an A/B test if the observations are extremely right-skewed?__  \n",
    "- Choose an appropriate metric:  \n",
    "Instead of using the mean, which can be heavily influenced by extreme values in right-skewed data, consider using the median or other robust metrics that are less sensitive to outliers.  \n",
    "- Transform the data:  \n",
    "Apply a transformation, such as the Box-Cox or log transformation, to reduce skewness and make the data more symmetric. This can help stabilize variance and make the data more suitable for statistical analysis.  \n",
    "- Use non-parametric tests:  \n",
    "Non-parametric tests, such as the Mann-Whitney U test or the Kolmogorov-Smirnov test, do not rely on assumptions about the underlying data distribution and can be more appropriate for analyzing skewed data.  \n",
    "- Bootstrap or permutation tests:  \n",
    "These resampling-based methods can provide valid inferences without relying on distributional assumptions. They can be particularly useful when dealing with skewed data or small sample sizes. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
