{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Cheatsheet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EDA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- covariance and correlation: shows the relationship between 2 variables.  \n",
    "corr: (-1,1)  \n",
    "<u>Pearson’s correlation</u> only measures linear relationships. If there’s a nonlinear relationship, we need other corr formulas.  \n",
    "we use _corr_ for _num features_ and _chi-square_ for _cat features_. _ANCOVA_ for both of them.\n",
    "\n",
    "- VIF:  \n",
    "to check multicolinearity. -> to solve it: use regularization  \n",
    "\n",
    "\n",
    "- mean and variance and std are sensitive to outliers, median and mean absolute deviation are not.\n",
    "\n",
    "\n",
    "\n",
    "- PCA:  \n",
    "_first drop correlated features, adding correlated variables lets PCA put more importance on those variable, which is misleading._\n",
    "<br>\n",
    "PCA transforms features in a dataset by combining them into uncorrelated linear combinations. These new features, or principal components, sequentially maximize the variance represented (i.e. the first principal component has the most variance, the second principal component has the second most, and so on)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">models ok with missing values:  \n",
    "_trees, naive bayes_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first of all, check if there's a pattern in missing values (For example, if a survey question was left unanswered), if it was fill all of them with same value,  \n",
    " otherwise:</font>\n",
    "\n",
    "\n",
    "1. median, mode, mean\n",
    "2. regressive (iterative): models like linear reg\n",
    "3. classifier: models like knn \n",
    "4. distance-based\n",
    "5. masking:  \n",
    "1 if it exists and 0 if not\n",
    "5. multiple:  \n",
    " creating multiple imputed datasets, each with a different set of imputed values, and then combining the results to obtain a final estimate. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Encoding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">models ok with cat data:  \n",
    "_Naive Bayes, tree-based (random forest, Catboost, LightGBM (not XGBoost))_\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. label \n",
    "2. One-hot\n",
    "3. binary\n",
    "4. count (frequency)\n",
    "5. Bucketizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outliers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">models robust to outliers:  \n",
    "_trees, GBM, SVM, NN_\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding outliers:\n",
    "1. IQR: using typical quantile, , good when data is not normally distibuted \n",
    "2. distribution: like z-score when data is normally distibuted (3.5 std away)\n",
    "3. models: \n",
    "    * isolation forest: good when outliers are in sparse regions\n",
    "    * LOF: density-based, good when outliers are in low density regions\n",
    "    * SVM, KDE\n",
    "\n",
    "\n",
    "Handling Outliers:\n",
    "1. Winsorize (cap at threshold).:\n",
    "    - the extreme values are replaced with the value at a specified percentile. For example, if we want to winsorize a dataset at the 5th and 95th percentiles, any values below the 5th percentile would be replaced with the value at the 5th percentile, and any values above the 95th percentile would be replaced with the value at the 95th percentile.\n",
    "2. Transform to reduce skew (using Box-Cox or similar):\n",
    "    - Stabilizing variance: stabilize the variance across different levels of the data, which can reduce the impact of outliers.\n",
    "    - Normalizing data: make the data more normally distributed, which can help reduce the influence of extreme values (outliers).\n",
    "    - Reducing skewness: reduce skewness in the data, which can make the data more symmetric and less sensitive to outliers\n",
    "3. Remove outliers if you're certain they are anomalies or measurement errors."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Filter based:  \n",
    "We specify some metric and based on that filter features. An example of such a metric could be correlation/chi-square(we calculate the chi-square metric between the target and the numerical variable and only select the variable with the maximum chi-squared values.).\n",
    "- Wrapper-based:  \n",
    "consider the selection of a set of features as a search problem. Example: Recursive Feature Elimination\n",
    "- Embedded:  \n",
    "algorithms that have built-in feature selection methods. For instance, Lasso and RF have their own feature selection methods.\n",
    "\n",
    "- Use Random Forest, Xgboost and plot variable importance chart\n",
    "- information gain (Mututal Information)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imbalance Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">models more robust to imbalance data:  \n",
    "_random forest, GBM, SVM, NN, KNN_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Resampling:  \n",
    "oversampling the minority class or undersampling the majority class.  \n",
    "Specifying a hypothesis and then collecting data following randomization and random sampling principles ensures against bias.\n",
    "2. Ensemble methods\n",
    "3. weight classes: penalized-SVM and penalized-LDA.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* __Frequentist__ (linear regression, logistic, svm, tree based, ...): for <u>large datas</u> , based on maximum likelihood\n",
    "* __Bayesian__ (bayesian linear regression, ...) -> for <u>small</u> and <u>noisy</u> data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Parametric models__:  \n",
    "finite number of parameters. To predict new data, you only need to know the parameters of the model.( linear reg, logistic reg, and linear SVMs.)\n",
    "* __Non-parametric models__:  \n",
    "an unbounded number of parameters, allowing for more flexibility. To predict new data, you need to know the parameters of the model and the state of the data that has been observed. ( trees, KNN, and topic models using latent dirichlet analysis.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Generative__:   \n",
    "_learn categories of data_  \n",
    "best for small datasets, generate new samples from the learned input <u>distributions</u> <br> \n",
    "                based on density, prior and posterior(GMM, Naive Bayes, variational autoencoders,...)<br>\n",
    "                give us more information, since they learn both the <u>input distribution</u> and the <u>class probabilities</u>.<br>\n",
    "                can deal naturally with missing data<br>\n",
    "                more sensitive to outliers, because outliers can have large effect on the input distributions.<br>\n",
    "\n",
    "* __Discriminative__:   \n",
    "    _learn the distinction between different categories of data_  \n",
    "    1.distribution-free (KNN, trees, SVM) 2. probabilistic based on posterior (logistic regression,NN,..)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* __Kernel-based__:  \n",
    "better learn non-linear datas  \n",
    "robust to noise  \n",
    "efficient when using kernel trick in higher spaces\n",
    "\n",
    "* __Ensemble models__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__loss functions__:\n",
    "* MSE, MAE\n",
    "* Huber Loss:  \n",
    "It uses MSE for small errors and MAE for large errors.  \n",
    "less sensitive to outliers\n",
    "\n",
    "\n",
    "for optimization: put deviations of loss function to zero"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "__Evaluation metrics__:\n",
    "1. MSE: easier to optimize, penalize large errors, absolute measure\n",
    "\n",
    "2. RMSE: penalize large errors, better for interpretation\n",
    "\n",
    "3. MAE: not good when outliers are prominent, more robust to outliers,\n",
    "           treats all errors the same,\n",
    "           doesn't change th units\n",
    "\n",
    "4. MAPE: (0-100) -> 0 is the best\n",
    "\n",
    "5. R-squared:  \n",
    "This metric represents the part of the variance of the dependent variable explained by the independent variables of the model  \n",
    "               not good for overfitting because of independent variable which make model complicated,  \n",
    "               so we need adjusted r- squared,(0-1) -> 1 is the best  \n",
    "               R-squared is better than RMSE, because RMSE is an absolute \n",
    "            measure (highly dependent on the variables, not a normalized measure)  \n",
    "               Example:  \n",
    "                    in marketing campaigns, where companies can measure their effectiveness by analyzing the relationship between the amount of money spent on advertising and the resulting increase in sales revenue. A high R-squared would indicate that there is a strong relationship between money spent on advertising and sales revenue"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what we must make sure of:\n",
    "- The errors or residuals of the data are normally distributed and independent from each other.\n",
    "- There is minimal multicollinearity between explanatory variables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __NN__:  \n",
    "The point of adding a Normalization layer to the architecture of a neural network is to improve its convergence. <br>\n",
    "The more faster we approach global minimum, the less time and resources we require for training. <br>\n",
    "Adding a normalization layer also helps in elevating the robustness of the model.<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Others__:  \n",
    "normalization should be used in distance-based algorithms (knn,kmeans, SVM, PCA...) and doesn't help with tree based models.\n",
    "\n",
    "    1. Z-Score Normalization (Standard Scaler): mean = 0 and std = 1\n",
    "\n",
    "    2. Min-Max Scaling : (0-1), useful when the distribution of features is somewhat <u>uniform</u> in nature\n",
    "\n",
    "    3. Log Scaling: helpful when there is a large imbalance in the distribution of the feature values,\n",
    "                    improving the linear performance of the model,\n",
    "    4. Robust: useful when the data contains outliers or extreme values that can affect the scaling of the data. \n",
    "           most used in algorithms sensitive to the scale of the data,  knn and svm.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> overfit: Low bias and high variance <br>\n",
    "\n",
    "* Ridge(L2): squared\n",
    "* Lasso(L1): absolute, since it can make some features to be 0 it's a feature selection\n",
    "* more data, less features, lower complexity\n",
    "* larger K in knn, lower depth in trees, tuning hyperparameters using techniques such as grid search, random search, or Bayesian optimization.\n",
    "* cross validation\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">In case of classification problem, we should always use __stratified sampling__ instead of random sampling. A random sampling doesn’t takes into consideration the proportion of target classes. On the contrary, stratified sampling helps to maintain the distribution of target variable in the resultant distributed samples also."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Linear classification: optimized by gradient descent, loss functions: hinge loss, logistic loss,... <br>\n",
    "* logistic regression: based on maximizing likelihood, loss: sigmoid, \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__loss functions__:\n",
    "* cross-entropy (binary/ categorical):   \n",
    "is a measure of the difference between two probability distributions.  \n",
    "                 for models that predict probabilities.  \n",
    "                 most suited for gradient descent using <u>logistic regression</u>.\n",
    "\n",
    "* Hinge Loss: \n",
    "    used in SVMs for binary classification.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Evaluation__:\n",
    "1. accuracy: TP + TN / total\n",
    "\n",
    "2. Percision: TP / TP + FP, useful when the cost of false positives is high (like fraud detection), not good in imbalance data\n",
    "\n",
    "3. Recall: TP / TP + FN, useful when the goal is to detect as many positive cases as possible. (like diseases)\n",
    "\n",
    "4. F1-score: 2*P*R / P + R, useful when both false positives and false negatives are important aspects to consider,( spam detection),  \n",
    "best for imbalanced data\n",
    "\n",
    "5. AUC: \n",
    "<br>\n",
    "(0-1) the more the better, generated by changing thresholed and calculating TPR and FPR, <b>robust to outliers</b>, only for binary classes,  \n",
    "Note that with highly imbalanced datasets, ROC AUC can be misleading.\n",
    "\n",
    "6. log loss:  useful when the goal is to penalize the model for being overly confident about predicting the wrong class"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Model__:  \n",
    ">for non-linear data bagging and boosting models are better.  \n",
    " for deployment simple models like linear reg,... are better than black box models like svm, xgbosst ,...\n",
    "\n",
    "\n",
    "* __maximal margin classifier__: base of SVM, sensitive to outliers\n",
    "* __SVM__:  \n",
    "using kernel functions (like radial basis) changes the space and dimensions to find new realtions  \n",
    "_Kernel_: A more complex kernel (e.g., RBF) can reduce bias but increase variance, while a simpler kernel (e.g., linear) can increase bias but reduce variance.   \n",
    "_Regularization parameter_ (C): Increasing C can lead to a more complex model, which reduces bias but increases variance.  \n",
    "* __KNN__:  \n",
    "the higher K the smoother bounderies,   \n",
    "        can handle <u>non-linear</u> relationships,  \n",
    "        not perform well with high-dimensional data due to the \"curse of dimensionality.\",  \n",
    "        <u>not good with outliers</u> because of instance-based method  \n",
    "        We don’t use manhattan distance because it calculates distance horizontally or vertically only. euclidean metric can be used in any space to calculate distance.  \n",
    "        _Number of neighbors_ (k): Increasing K can lead to a smoother decision boundary, which increases bias but reduces variance. \n",
    "\n",
    "* __KDE__:  \n",
    "useful when the data is not well-represented by a parametric distribution,  \n",
    "        <u>robust</u> when dealing with small or noisy datasets,   \n",
    "        helpful in identifying outliers or anomalies.\n",
    "* __Naive Bayes__: \n",
    "\n",
    "* __Decision Trees__:  \n",
    "_Max depth_: Increasing the maximum depth of the tree can lead to a more complex model, which reduces bias but increases variance.  \n",
    "_Min samples split_: Increasing the minimum number of samples required to split a node can lead to a simpler model, which increases bias but reduces variance.\n",
    "\n",
    "* __Logistic Reg__: robust to outliers\n",
    "\n",
    "* __Neural Networks__:  \n",
    "the main motivation for using activation functions in NN is Capturing complex non-linear patterns.  \n",
    "_Number of hidden layers and neurons_: Increasing the number of hidden layers or neurons can lead to a more complex model, which reduces bias but increases variance.  \n",
    "_Regularization techniques_ (e.g., L1, L2, or dropout): Applying regularization can help control the complexity of the model, reducing variance while potentially increasing bias.\n",
    "\n",
    "\n",
    "* __Ensemble methods__ (e.g., Random Forests, Gradient Boosting):   \n",
    " _Number of base models_: Increasing the number of base models can help reduce variance by averaging the predictions of multiple models. However, it may not significantly affect bias.  \n",
    "_Hyperparameters of base models_: Changing the hyperparameters of the base models can affect the bias and variance of the ensemble method.  \n",
    "larger trees in random forest causes overfit.\n",
    "\n",
    "* __Random Forest__:  \n",
    "Calibration in Random Forest refers to the process of adjusting the predicted probabilities of the model to better match the true probabilities of the outcome  \n",
    "  methods:  \n",
    "  - Platt scaling: This method fits a logistic regression model to the predicted probabilities of the Random Forest model and uses the coefficients to adjust the probabilities.  \n",
    "  - Isotonic regression: This method fits a non-parametric regression model to the predicted probabilities and adjusts them to be monotonic.  \n",
    "  - Bayesian calibration: This method uses a Bayesian framework to estimate the true probabilities of the outcome and adjusts the predicted probabilities accordingly.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Density Estimation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a technique used to estimate the probability density function(PDF) of a random variable based on a set of observed data. \n",
    "- Kernel density estimation:  \n",
    "a non-parametric technique that estimates PDF by placing a kernel function at each data point and summing the contributions of all kernels.  \n",
    "The bandwidth of the kernel function determines the smoothness of the estimated density.\n",
    "- Gaussian mixture models:  \n",
    "a parametric technique that models the PDF as a weighted sum of Gaussian distributions.  \n",
    "The number of Gaussian components and their parameters are estimated from the observed data.\n",
    "- Autoencoders:  \n",
    "a neural network architecture that can be used for density estimation by training the network to reconstruct the input data.  \n",
    "The network can be used to estimate the PDF by measuring the reconstruction error.\n",
    "- Variational autoencoders:  \n",
    "a type of autoencoder that can be used for generative modeling and density estimation.  \n",
    "Variational autoencoders learn a latent representation of the data and use it to generate new samples that are similar to the observed data.\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Theory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. __Entropy__:  \n",
    "can be used to quantify the amount of information contained in a dataset or a feature (decision trees for splitting nodes)\n",
    "2. __Mutual Information__:  \n",
    "measure of the amount of information that two random variables share.\n",
    "                     (feature selection: subset of features that maximizes the mutual information between the features and the target variable.)\n",
    "3. __Kullback-Leibler Divergence__:  \n",
    "a measure of the difference between two probability distributions. \n",
    "                                used to compare the similarity of two models or to optimize the parameters of a model.  \n",
    "                                (unsupervised learning: to measure the difference between the input data distribution and the model distribution, and the goal is to minimize the KL divergence by adjusting the model parameters.)  \n",
    "                                __KL loss__:   \n",
    "is a measure of the difference between two probability distributions.(it uses log in formula)  \n",
    "measure the difference between the prior distribution and the posterior distribution of the model parameter  \n",
    "           for models that predict probability distributions.   \n",
    "           useful in <u>generative models</u>, such as VAEs, to train the model to generate samples that are similar to the true data distribution. \n",
    "4. __Information Gain__:  \n",
    "Information gain is a measure of the reduction in entropy,\n",
    "                     can be used to select the best feature to split a node."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">hyperparameter: learning rate\n",
    "\n",
    "* Full Batch Gradient Descent:  \n",
    "        computing the gradient of the cost function using the entire training dataset.\n",
    "        No shuffle is needed, because each update passes through the entire dataset anyway and the order doesn't matter. \n",
    "* Stochastic Gradient Descent:  \n",
    "        computing the gradient using a single randomly selected training example at a time. \n",
    "* Mini-batch Gradient Descent:  \n",
    "        is a compromise between the two, using a small randomly selected subset of the training dataset\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Models__:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">instead of train-test splitting, clustering algorithms typically use all available data.\n",
    "However, some clustering algorithms may require tuning of hyperparameters. In these cases, it may be useful to use cross-validation.  \n",
    "\n",
    "\n",
    ">In clustering, bias refers to the tendency of a model to consistently miss the true structure of the data\n",
    "variance refers to the tendency of a model to be overly sensitive to noise in the data.  \n",
    "bias can arise when the algorithm is not flexible enough to capture the true structure of the data. For example, if the algorithm assumes that the data is linearly separable when it is actually non-linear, it will consistently miss the true structure of the data. This can lead to underfitting, where the model is too simple to capture the complexity of the data.  \n",
    "On the other hand, variance can arise when the algorithm is too flexible and captures noise in the data instead of the true structure. \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "density-based and model-based\n",
    "1. __KMeans__:  \n",
    "find k by variance reduction -> elbow method,   \n",
    "        suitable for datasets with a large number of features and a moderate number of clusters,  \n",
    "        works well when the clusters are well-separated and have a roughly spherical shape.  \n",
    "        regularization makes the decision boundaries more regular.  \n",
    "   * _Using cosine distance as the distance metric in clustering_:  \n",
    "     - useful in situations where the magnitude of the feature vectors is not important, and only the direction of the vectors matters. For example, in text clustering, the cosine distance can be used to measure the similarity between two documents based on the angle between their word frequency vectors.  \n",
    "     - less sensitive to the scale of the input features compared to Euclidean distance.  \n",
    "     - more effective at capturing the semantic similarity between data points. This is because the cosine distance measures the similarity between the directions of the feature vectors, which can be more meaningful in some applications than the distance between the feature values themselves.  \n",
    "     - one potential disadvantage is that it can be sensitive to the sparsity of the input features. In high-dimensional spaces, many feature vectors can be orthogonal to each other, resulting in a cosine distance of zero. This can make it difficult to distinguish between data points that are far apart in the feature space but have similar directions\n",
    "2. __Hierarchial__:  \n",
    "suitable for datasets with a small to moderate number of features and a small to moderate number of clusters.\n",
    "\n",
    "3. __GMM__ :  \n",
    "modeling complex data distributions,   \n",
    "         work well when the clusters have different shapes and sizes,  \n",
    "         can be used for density estimation and anomaly detection.\n",
    "4. __DBSCAN__:  \n",
    "(density-based:  density refers to the concentration of data points in a given area or region.)  \n",
    "           suitable for datasets with arbitrary shapes, densities and sizes of clusters,(spatial data such as the geometrical locations of houses)  \n",
    "           handle noise and outliers in the data.\n",
    "\n",
    "5. __Fuzzy C-Means__:  \n",
    "suitable for datasets where each data point can belong to multiple clusters with different degrees of membership.\n",
    "                 works well when the clusters have overlapping boundaries and can be used for feature selection and data compression.\n",
    "\n",
    "6. __LDA__: generative, best for topic modeling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Evaluation__:\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Internal:\n",
    "    1. Silhouette coefficient:  \n",
    "    measures the similarity of a data point to its own cluster compared to other clusters. The higher the better. \n",
    "    2. Calinski-Harabasz index:  \n",
    "    measures the ratio of the between-cluster variance to the within-cluster variance. The higher the better.\n",
    "    3. davies-Bouldin index:  \n",
    "    The Davies-Bouldin index measures the average similarity between each cluster and its most similar cluster. The lower the better.  \n",
    "\n",
    "* External:  \n",
    "    1. Adjusted Rand index:  \n",
    "    measures the similarity between the clustering results and the ground truth labels. The higher the better.\n",
    "    2. Fowlkes-Mallows index:  \n",
    "    measures the geometric mean of the precision and recall of the clustering results compared to the ground truth labels. The higher the better.\n",
    "    3. Normalized Mutual Information:  \n",
    "    measures the mutual information between the clustering results and the ground truth labels, normalized by the entropy of the two sets. The higher the better."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">reduce overfitting, more robust  \n",
    "ensemble learners are built on the premise of combining weak <b>uncorrelated</b> models. if models are correlated, no improve will be made.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. __averaging__:  \n",
    "              Averaging models improve the performance and stability and a lower variance than individual models   \n",
    "              by combining the predictions of multiple models.   \n",
    "              _simple averaging_, _weighted averaging_, _stacking_(This involves training a meta-model that takes the predictions of multiple models   as input and produces a final prediction.)  \n",
    "              1. Split the training data into two or more parts.  \n",
    "              2. Train several base models on the first part of the training data.  \n",
    "              3.Use the trained base models to make predictions on the second part of the training data.  \n",
    "              4. Combine the predictions of the base models to create a new dataset.  \n",
    "              5. Train a meta-model on the new dataset, using the actual target values as the labels.  \n",
    "              6. Use the trained meta-model to make predictions on the test data.  \n",
    "\n",
    "2. __Bagging__:  \n",
    "            (random forest) reduce variance by taking a random sample of data,  \n",
    "            runs weighted averages in <u>parallel</u>,  \n",
    "             useful when the base models are <u>unstable</u>, meaning that small changes in the training data can lead to large changes in the model.   \n",
    "             useful when the base models are <u>simple</u>, as it can help to reduce bias and improve model performance.  \n",
    "             predictions are combined using <u>voting</u> (classification) or <u>averaging</u> (regression).  \n",
    "\n",
    "3. __Boosting__:  \n",
    "            (ada boost, catboost) These trees would be working in a sequential order.   \n",
    "             The output of one tree is used by other trees to focus more on the errors and to fit over the residuals.  \n",
    "             useful when the base models are <u>weak</u>, meaning that they have low predictive power  \n",
    "             useful when the base models are <u>complex</u>  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semi-Supervised"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">involves training models on a combination of labeled and unlabeled data.   \n",
    "Self-organizing maps are specialized neural network for semi-supervised learning.\n",
    "\n",
    "\n",
    "- Self-training:  \n",
    " involves training a model on the labeled data, and then using the model to make predictions on the unlabeled data. The high-confidence predictions are then added to the labeled data, and the model is retrained on the expanded labeled data.\n",
    "- Co-training:   \n",
    "involves training two or more models on different subsets of the features. The models make predictions on the unlabeled data, and the high-confidence predictions are used to train the other models.\n",
    "- Generative models:   \n",
    "Generative models, such as generative adversarial networks (GANs) and variational autoencoders (VAEs), can be used for semi-supervised learning by learning to generate realistic samples from the data distribution.  \n",
    "The generative models can be trained on the labeled and unlabeled data, and the generated samples can be used to improve the performance of the discriminative models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, online learning is a machine learning technique that allows a model to learn continuously from new data as it becomes available, without requiring the model to be retrained from scratch. Online learning is particularly useful in situations where the data is constantly changing or evolving, and has several advantages over traditional batch learning, including scalability, adaptability, efficiency, and cost-effectiveness."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Models__:\n",
    "* model-based:\n",
    "        uses the transition function (and the reward function) in order to estimate the optimal policy.\n",
    "* model-free: \n",
    "        estimates the optimal policy without using or estimating the dynamics (transition and reward functions) of the environment.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Q-learning: model-free\n",
    "    more: https://towardsdatascience.com/a-beginners-guide-to-q-learning-c3e2a30a653c#:~:text=Q%2Dlearning%20is%20a%20model,equation(particularly%20Bellman%20equation).&text=Means%20it%20learns%20the%20value,independently%20of%20the%20agent's%20actions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommender Systems"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filtering:\n",
    "* content-based\n",
    "* collaborative\n",
    "* Hybrid\n",
    "\n",
    "Vectorization:\n",
    "* tf-idf\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    ">In time series problem, k fold can be troublesome because there might be some pattern in year 4 or 5 which is not in year 3.Instead, we can use forward chaining strategy\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
